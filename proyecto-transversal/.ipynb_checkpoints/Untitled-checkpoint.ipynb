{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> EL Terrible notebook de la tarea final </h1> \n",
    "\n",
    "<h3> Pregunta de Investigación: </h3>\n",
    "\n",
    "<p>¿Son identificables las posturas presentes en noticias del cambio climático de medios de prensa chilena con un análisis de tópicos? </p>\n",
    "\n",
    "<h3> Hipótesis: </h3>\n",
    "<ul>\n",
    "    <li>A: Podemos encontrar tópicos representativos de posturas frente al cambio climático en el dataset de medios de prensa chilenos con análisis de tópicos. </li>\n",
    "    <li>B: Existen tópicos clave en el dataset que se asocian a una postura en el cambio climático.</li>\n",
    "    </ul>\n",
    "    \n",
    "    \n",
    "\n",
    "<h2> Metodología: </h2>\n",
    "\n",
    "<h2 align=\"center\"> </h2>\n",
    "\n",
    "<h2> Caracterización de los datos: </h2>\n",
    "\n",
    "<h2 align=\"center\"> </h2>\n",
    "\n",
    "<h2> Análisis primeros resultados: </h2>\n",
    "\n",
    "<h2 align=\"center\"> </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1235"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Cargar el dataset de tweets\n",
    "df_cclimatico = pd.read_csv('datasets/sophia_cambioclimatico_v2.csv',delimiter=\"|\", header=None)\n",
    "df_cclimatico\n",
    "\n",
    "#selección de los mensajes\n",
    "docs = df_cclimatico[3].as_matrix()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Explorar los datos </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/alan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/alan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "# load nltk's Spanish stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11374.201137838794\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: loading working playlists ¿qué vergara podemos frente disminuido acción regional hacer plan laguna peter observar tema enfrentan comité corecc regionales paine reunió medidas mitigación implementación diego internacional terrestres funcionamiento hombre\n",
      "Topic #1: login slide /a br show view href= ago socializer specify section admin x super work in key googleplus linkedin leave caleta algarrobo id consumer api bionoticiascl frutillar quisco coliumo secret\n",
      "Topic #2: minutos dijo energía universidad decisión aumento email agricultura millones mar dos personas medidas especies científicos hoy fenómeno bachelet cómo producción futuro así temperatura plan hacer internacional temperaturas estudio tiempo puede\n",
      "Topic #3: minutos educación piñera sexual hora leyendo libertadores horas videos regional nueva aguas universidad mena riego mar comisión reunión indap estudio presidenciales seguir seminario temas candidatos huracanes científicos isla investigación salud\n",
      "Topic #4: hora wwf edificios luces señal acción marzo campaña adoptando voluntaria promovida sábado conéctate asimismo real ciudades iniciativa aparatos ambiental llamado bosshard torre esenciales icónicos sufrida eléctricos invita prescindibles aludiendo emblemáticos\n",
      "Topic #5: sídney apagarán monumentos luces pirámides edición atenas acrópolis kremlin numerosas egipcias alhambra granada recordará siddarth preocupaban organizadora das únicamente lanzamos movimiento pekín velas tradicional empezando tanzania concierto singapur empezaron simbólico\n",
      "Topic #6: not was to the orrego an on intendente use educación hora url andinas agricultura error innovación this newsletter servicio empresa metropolitana lluvias zona josé plantas aguas potable medios actuar valor\n",
      "Topic #7: servicio boscosos velar mantención deberá recuperación asumir urgencia prevenir marcados catástrofes cuidado contexto wwf incendios diversos desafíos ecosistemas nuevo entusiasmo envío resume facultades recibido creará tareas institución contará conaf puntos\n",
      "Topic #8: newsletter ciudad guioteca hoyxhoy.cl farox elige contáctenos ¿cuándo inmobiliaria soychile.cl entérate quieres emol correos propiedades recibirás sucede autolocal.cl suscríbete anunciar pronto recibir valor condiciones términos futuro sa periodística araucanía gestión\n",
      "Topic #9: vulnerables artículo cómo prevenir maule concepción nobel newsletter velar video posibilidad malo animales cine saben mencionado prevención nueva definitivamente anteriormente continente crítica aquello origen of metropolitana impuestos partes sexual meza\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Aplicando Modelos Probabilistas de Tópicos y LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer(max_df=0.2, min_df=7,\n",
    "                                stop_words=stopwords,tokenizer=tokenize_only, ngram_range=(1,1))\n",
    "tf = tf_vectorizer.fit_transform(docs)\n",
    "diccionario= tf_vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "#Estimación de LDA con Bayes Variacional\n",
    "lda = LatentDirichletAllocation(n_components=10, max_iter=10,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "lda.fit(tf)\n",
    "#Cálculo de índice de ajuste de los datos\n",
    "print(lda.perplexity(tf))\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "print_top_words(lda, diccionario, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hola', 'hola', 'NOUN'), ('como', 'comer', 'SCONJ'), (',', ',', 'PUNCT'), ('estas', 'este', 'PRON'), ('5', '5', 'NUM')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es')\n",
    "doc = nlp(\"hola como, estas 5\")\n",
    "print([(w.text,w.lemma_, w.pos_) for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
